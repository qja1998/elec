{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('preproc_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(A, F):\n",
    "    return 100 / len(A) * np.sum(np.abs(F - A) / (np.abs(A) + np.abs(F)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, cuda_flag=False, bidirectional=False):\n",
    "        # assumes that batch_first is always true\n",
    "        super(TimeLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.cuda_flag = cuda_flag\n",
    "        self.W_all = nn.Linear(hidden_size, hidden_size * 4)\n",
    "        self.U_all = nn.Linear(input_size, hidden_size * 4)\n",
    "        self.W_d = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "    def forward(self, inputs, timestamps, lens, reverse=False):\n",
    "        # inputs: [b, seq, embed]\n",
    "        # h: [b, hid]\n",
    "        # c: [b, hid]\n",
    "        b, seq, embed = inputs.size()\n",
    "        h = torch.zeros(b, self.hidden_size, requires_grad=False)\n",
    "        c = torch.zeros(b, self.hidden_size, requires_grad=False)\n",
    "        if self.cuda_flag:\n",
    "            h = h.cuda()\n",
    "            c = c.cuda()\n",
    "        outputs = []\n",
    "        for s in range(seq):\n",
    "            c_s1 = torch.tanh(self.W_d(c))\n",
    "            c_s2 = c_s1 * timestamps[:, s:s + 1].expand_as(c_s1)\n",
    "            c_l = c - c_s1\n",
    "            c_adj = c_l + c_s2\n",
    "            outs = self.W_all(h) + self.U_all(inputs[:, s])\n",
    "            f, i, o, c_tmp = torch.chunk(outs, 4, 1)\n",
    "            f = torch.sigmoid(f)\n",
    "            i = torch.sigmoid(i)\n",
    "            o = torch.sigmoid(o)\n",
    "            c_tmp = torch.sigmoid(c_tmp)\n",
    "            c = f * c_adj + i * c_tmp\n",
    "            h = o * torch.tanh(c)\n",
    "            outputs.append(h)\n",
    "        if reverse:\n",
    "            outputs.reverse()\n",
    "        outputs = torch.stack(outputs, 1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
