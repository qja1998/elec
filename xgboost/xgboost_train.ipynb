{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01de2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- Python & library version --------------------------\n",
    "# Python version: 3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]\n",
    "# pandas version: 1.2.4\n",
    "# numpy version: 1.19.2\n",
    "# matplotlib version: 3.3.4\n",
    "# tqdm version: 4.59.0\n",
    "# sktime version: 0.6.1\n",
    "# xgboost version: 1.2.1\n",
    "# seaborn version: 0.11.1\n",
    "# scikit-learn version: 0.24.2\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "civilian-stations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- Python & library version --------------------------\n",
      "Python version: 3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]\n",
      "pandas version: 1.2.4\n",
      "numpy version: 1.19.2\n",
      "matplotlib version: 3.3.4\n",
      "tqdm version: 4.59.0\n",
      "sktime version: 0.6.1\n",
      "xgboost version: 1.2.1\n",
      "seaborn version: 0.11.1\n",
      "scikit-learn version: 0.24.2\n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import sktime\n",
    "import tqdm as tq\n",
    "import xgboost as xgb\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "print(\"-------------------------- Python & library version --------------------------\")\n",
    "print(\"Python version: {}\".format(sys.version))\n",
    "print(\"pandas version: {}\".format(pd.__version__))\n",
    "print(\"numpy version: {}\".format(np.__version__))\n",
    "print(\"matplotlib version: {}\".format(matplotlib.__version__))\n",
    "print(\"tqdm version: {}\".format(tq.__version__))\n",
    "print(\"sktime version: {}\".format(sktime.__version__))\n",
    "print(\"xgboost version: {}\".format(xgb.__version__))\n",
    "print(\"seaborn version: {}\".format(sns.__version__))\n",
    "print(\"scikit-learn version: {}\".format(skl.__version__))\n",
    "print(\"------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6b5e25e-f9a1-4980-ac53-6cdb73b3d0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.utils.plotting import plot_series\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "pd.set_option('display.max_columns', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83e7738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/preproc3_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c414fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('Time', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffe02f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['building_id', 'sin_time', 'cos_time', 'day', 'month', 'day_hour_mean',\n",
       "       'hour_mean', 'hour_std', 'holiday', 'Temperature', 'Precipitation',\n",
       "       'Wind speed', 'Humidity', 'DI', 'Solar radiation', 'consumption',\n",
       "       'CDH'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d356ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_dim = 1\n",
    "\n",
    "# class Regressor(torch.nn.Module):\n",
    "#     def __init__(self, input_size, em_size=16, output_size=12, a=10):\n",
    "#         super().__init__()\n",
    "#         self.linear = nn.Sequential(\n",
    "#             nn.Linear(input_size, em_size),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(em_size, output_size),\n",
    "#             nn.Softmax(),\n",
    "#         )\n",
    "#         self.reg = XGBRegressor(n_estimators=a, tree_method='gpu_hist', objective = 'multi:softprob')\n",
    "\n",
    "#     def forward(self, x, y, train=True):\n",
    "#         x = self.linear(x)\n",
    "        \n",
    "#         if train:\n",
    "#           self.reg.fit(x.detach().cpu().numpy(), y.detach().cpu().numpy())\n",
    "          \n",
    "#         out = self.reg.predict(x.detach().cpu().numpy())\n",
    "#         out = torch.tensor(out)\n",
    "\n",
    "#         return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dabc56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train embedding models\n",
    "# t_input = len(train.columns) - 1\n",
    "# s_input = len(train.columns) - 2\n",
    "# output_size = 15\n",
    "# epochs = 100\n",
    "\n",
    "# t_embedding = EmbeddingModel(t_input)\n",
    "# s_embedding = EmbeddingModel(s_input)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = nn.optim.Adam(s_embedding.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "personal-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SMAPE loss function\n",
    "def SMAPE(true, pred):\n",
    "    return np.mean((np.abs(true-pred))/(np.abs(true) + np.abs(pred))) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c131d074-ca9f-4c1b-824b-9899c4442c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### alpha를 argument로 받는 함수로 실제 objective function을 wrapping하여 alpha값을 쉽게 조정할 수 있도록 작성했습니다.\n",
    "# custom objective function for forcing model not to underestimate\n",
    "def weighted_mse(alpha = 1):\n",
    "    def weighted_mse_fixed(label, pred):\n",
    "        residual = (label - pred).astype(\"float\")\n",
    "        grad = np.where(residual>0, -2*alpha*residual, -2*residual)\n",
    "        hess = np.where(residual>0, 2*alpha, 2.0)\n",
    "        return grad, hess\n",
    "    return weighted_mse_fixed\n",
    "\n",
    "def log_mae(label, pred):\n",
    "    ae = np.log1p(label) - np.log1p(pred)\n",
    "    grad = -ae / (label * abs(ae))\n",
    "    hess = ae / (label ** 2 * abs(ae))\n",
    "    return grad, hess\n",
    "\n",
    "def rmsle_loss(y_pred, y_true):\n",
    "    return 'rmsle', np.sqrt(np.mean((np.log1p(y_pred) - np.log1p(y_true))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcca426c-ed3a-4bd2-b0fe-c700b29c080b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47791449e5e942dc83134f0cf87d647f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 1 SMAPE : 8.153339110837528\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 5, 'min_child_weight': 6, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 2 SMAPE : 6.544723905683142\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 3 SMAPE : 5.369298929857689\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 8, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 4 SMAPE : 5.520829567827768\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 6, 'min_child_weight': 5, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 5 SMAPE : 8.635351245844769\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 8, 'min_child_weight': 6, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 6 SMAPE : 8.083842870961762\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 3, 'min_child_weight': 4, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 7 SMAPE : 5.884313073628173\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 5, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 8 SMAPE : 7.320269137724076\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 5, 'min_child_weight': 5, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 9 SMAPE : 7.577948284910225\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 3, 'min_child_weight': 6, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 10 SMAPE : 4.57133146328079\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 11 SMAPE : 6.616861695143837\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 12 SMAPE : 8.257716798439134\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 4, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 13 SMAPE : 8.402359238694007\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 14 SMAPE : 8.348223322971416\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 6, 'min_child_weight': 5, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 15 SMAPE : 6.630765973155499\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 6, 'min_child_weight': 6, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 16 SMAPE : 6.4883329283172255\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 17 SMAPE : 8.683076928894867\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 6, 'min_child_weight': 6, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 18 SMAPE : 9.202877137702076\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 8, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 19 SMAPE : 9.508711055018654\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 20 SMAPE : 8.22836088450394\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 5, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 21 SMAPE : 7.935836704853879\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 5, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 22 SMAPE : 7.540058846525197\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 23 SMAPE : 7.794351682136444\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 24 SMAPE : 7.405274017134074\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 5, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 25 SMAPE : 8.144584928252522\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 4, 'min_child_weight': 6, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 26 SMAPE : 7.35084944640578\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 27 SMAPE : 6.873050731308081\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 4, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 28 SMAPE : 4.48086413741095\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 4, 'min_child_weight': 6, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 29 SMAPE : 6.20374265591519\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 30 SMAPE : 7.63391760646653\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 7, 'min_child_weight': 6, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 31 SMAPE : 5.6036823672905856\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 32 SMAPE : 7.060749850703456\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 33 SMAPE : 7.4164518278936225\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 34 SMAPE : 7.160634915730769\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 35 SMAPE : 7.293336177231407\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 36 SMAPE : 7.313937481847618\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 7, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 37 SMAPE : 8.53726812784444\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 6, 'min_child_weight': 2, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 38 SMAPE : 7.4472787443990445\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 39 SMAPE : 9.459852901286672\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 4, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 40 SMAPE : 10.390206021434912\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 5, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 41 SMAPE : 7.496895367978695\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 6, 'min_child_weight': 6, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 42 SMAPE : 10.522648280357487\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 8, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 43 SMAPE : 7.627059127021167\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 5, 'min_child_weight': 5, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 44 SMAPE : 5.972680232786559\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 45 SMAPE : 8.096879008763509\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 7, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 46 SMAPE : 7.377780865057527\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 7, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 47 SMAPE : 6.119462241232804\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 6, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 48 SMAPE : 7.734659432871077\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 6, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 49 SMAPE : 8.882814188751908\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 8, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 50 SMAPE : 6.93823889728055\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 6, 'min_child_weight': 6, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 51 SMAPE : 8.455204239670607\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 7, 'min_child_weight': 4, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 52 SMAPE : 8.269081244276052\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 3, 'min_child_weight': 4, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 53 SMAPE : 6.748041964814735\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 4, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 54 SMAPE : 13.599041736158926\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 55 SMAPE : 7.283436896243903\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 56 SMAPE : 7.127494169520489\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 5, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 57 SMAPE : 7.304024857798994\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 3, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 58 SMAPE : 7.394229451306917\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 3, 'min_child_weight': 4, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 59 SMAPE : 6.588125444412237\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 60 SMAPE : 6.6317949060596835\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 5, 'min_child_weight': 2, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 61 SMAPE : 6.586322012633963\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 62 SMAPE : 6.359727212673964\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 5, 'min_child_weight': 5, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 63 SMAPE : 6.698020301576742\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 8, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 64 SMAPE : 6.33810793233862\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 6, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 65 SMAPE : 6.928804351658743\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 8, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 66 SMAPE : 8.09048792439007\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 8, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 67 SMAPE : 3.8145675133909402\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 5, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 68 SMAPE : 6.025413828378934\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 6, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 69 SMAPE : 6.565950821514225\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 3, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 70 SMAPE : 10.437225965515887\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 5, 'min_child_weight': 4, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 71 SMAPE : 8.628910093706319\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 4, 'min_child_weight': 6, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 72 SMAPE : 8.760226268309497\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 5, 'min_child_weight': 6, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 73 SMAPE : 6.898061312476693\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 3, 'min_child_weight': 6, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 74 SMAPE : 8.719638313707229\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 5, 'min_child_weight': 2, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 75 SMAPE : 7.7425443622890295\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 76 SMAPE : 7.986373522732649\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 5, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 77 SMAPE : 7.492440843232295\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 6, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 78 SMAPE : 8.295094324817924\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 79 SMAPE : 7.6843018823524165\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 8, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 80 SMAPE : 8.517367725762231\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 4, 'min_child_weight': 5, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 81 SMAPE : 7.226468418358027\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 3, 'min_child_weight': 6, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 82 SMAPE : 7.123453147817685\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 83 SMAPE : 7.659802490858575\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 3, 'min_child_weight': 6, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 84 SMAPE : 7.627500891844344\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 4, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 85 SMAPE : 7.941507629317438\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 3, 'min_child_weight': 6, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 86 SMAPE : 8.78374147923643\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 3, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 87 SMAPE : 9.105951380087012\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 8, 'min_child_weight': 2, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 88 SMAPE : 9.134188117314304\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 8, 'min_child_weight': 5, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 89 SMAPE : 9.044012555365253\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.8, 'eta': 0.01, 'max_depth': 8, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 90 SMAPE : 10.287915818610193\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 6, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 91 SMAPE : 10.088710897629092\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 92 SMAPE : 7.0247270250338865\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 93 SMAPE : 7.677472152819333\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 94 SMAPE : 8.126718679346487\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 95 SMAPE : 6.651629138161855\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 7, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 96 SMAPE : 6.638888356592261\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 8, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 97 SMAPE : 8.497759367736652\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 6, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 98 SMAPE : 7.633968147778194\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 7, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Building 99 SMAPE : 6.751278690334228\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 8, 'min_child_weight': 7, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 100 SMAPE : 8.534238700060866\n"
     ]
    }
   ],
   "source": [
    "## gridsearchCV for best model : 대략 1시간 소요\n",
    "\n",
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "smape = make_scorer(SMAPE, greater_is_better = False)\n",
    "\n",
    "df = pd.DataFrame(columns = ['n_estimators', 'eta', 'min_child_weight','max_depth', 'colsample_bytree', 'subsample'])\n",
    "preds = np.array([])\n",
    "\n",
    "grid = {'n_estimators' : [200], 'eta' : [0.01], 'min_child_weight' : np.arange(1, 8, 1), \n",
    "        'max_depth' : np.arange(3,9,1) , 'colsample_bytree' :np.arange(0.8, 1.0, 0.1), \n",
    "        'subsample' :np.arange(0.8, 1.0, 0.1)} # fix the n_estimators & eta(learning rate)\n",
    "        \n",
    "for i in tqdm(np.arange(1, 101)):\n",
    "    y = train.loc[train.building_id == i + 1, 'consumption']\n",
    "    x = train.loc[train.building_id == i + 1, ].drop(['building_id', 'consumption'], axis=1)\n",
    "    y_train, y_test, x_train, x_test = temporal_train_test_split(y = y, X = x, test_size = 168)\n",
    "    \n",
    "    \n",
    "    pds = PredefinedSplit(np.append(-np.ones(len(x_train)-168), np.zeros(168)))\n",
    "    gcv = GridSearchCV(estimator = XGBRegressor(seed = 0, gpu_id = 0, \n",
    "                                                tree_method = 'gpu_hist', predictor= 'gpu_predictor'),\n",
    "                       param_grid = grid, scoring = smape, cv=pds, refit = True, verbose = True)\n",
    "    \n",
    "    \n",
    "    gcv.fit(x_train, y_train)\n",
    "    best = gcv.best_estimator_\n",
    "    params = gcv.best_params_\n",
    "    print(params)\n",
    "    pred = best.predict(x_test)\n",
    "    print(f'Building {i} SMAPE : {SMAPE(y_test, pred)}')\n",
    "    preds = np.append(preds, pred)\n",
    "    df = pd.concat([df, pd.DataFrame(params, index = [0])], axis = 0)\n",
    "df.to_csv('./hyperparameter_teacher1.csv', index = False) # save the tuned parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d5696e3-5111-4e09-ad31-1156aa6af19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = pd.read_csv('./hyperparameter_teacher1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd0bf02-4a3b-4c0c-aa91-d4580d4773d3",
   "metadata": {},
   "source": [
    "### find the bset iteration (given alpha = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "proved-chain",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d38d6146e84b9d9f64b638f6441e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = []   # smape 값을 저장할 list\n",
    "best_it = []  # best interation을 저장할 list\n",
    "for i in tqdm(range(100)):\n",
    "    y = train.loc[train.building_id == i+1, 'consumption']\n",
    "    x = train.loc[train.building_id == i+1, ].drop(['building_id', 'consumption'], axis=1)\n",
    "    y_train, y_valid, x_train, x_valid = temporal_train_test_split(y = y, X = x, test_size = 168)\n",
    "    \n",
    "    xgb_reg = XGBRegressor(n_estimators = 20000, eta = 0.01, min_child_weight = xgb_params.iloc[i, 2],\n",
    "                           max_depth = xgb_params.iloc[i, 3], colsample_bytree = xgb_params.iloc[i, 4], \n",
    "                           subsample = xgb_params.iloc[i, 5], seed=0,\n",
    "                           tree_method = 'gpu_hist', predictor= 'gpu_predictor')\n",
    "    # xgb_reg.set_params(**{'objective':weighted_mse(100)}) # alpha = 100으로 고정\n",
    "    xgb_reg.set_params(**{'objective':'reg:squaredlogerror'}) # SMSLE\n",
    "    xgb_reg.fit(x_train, y_train, eval_set=[(x_train, y_train), \n",
    "                                            (x_valid, y_valid)], early_stopping_rounds=3000, verbose=False)\n",
    "    y_pred = xgb_reg.predict(x_valid)\n",
    "    pred = pd.Series(y_pred)   \n",
    "    \n",
    "    sm = SMAPE(y_valid, y_pred)\n",
    "    scores.append(sm)\n",
    "    # print(xgb_reg.best_iteration, sm)\n",
    "    best_it.append(xgb_reg.best_iteration) ## 실제 best iteration은 이 값에 +1 해주어야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-copper",
   "metadata": {},
   "source": [
    "### alpha tuning for weighted MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-plaza",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = []\n",
    "smape_list = []\n",
    "for i in tqdm(range(100)):\n",
    "    y = train.loc[train.building_id == i+1, 'consumption']\n",
    "    x = train.loc[train.building_id == i+1, ].drop('consumption', axis=1)\n",
    "    y_train, y_test, x_train, x_test = temporal_train_test_split(y = y, X = x, test_size = 168)\n",
    "    xgb = XGBRegressor(seed = 0,\n",
    "                      n_estimators = best_it[i], eta = 0.01, min_child_weight = xgb_params.iloc[i, 2],\n",
    "                      max_depth = xgb_params.iloc[i, 3], colsample_bytree = xgb_params.iloc[i, 4], subsample = xgb_params.iloc[i, 5],\n",
    "                      tree_method = 'gpu_hist', predictor= 'gpu_predictor')\n",
    "    \n",
    "    xgb.fit(x_train, y_train)\n",
    "    pred0 = xgb.predict(x_test)\n",
    "    best_alpha = 0\n",
    "    score0 = SMAPE(y_test,pred0)\n",
    "    \n",
    "    for j in [1, 3, 5, 7, 10, 25, 50, 75, 100]:\n",
    "        xgb = XGBRegressor(seed = 0,\n",
    "                      n_estimators = best_it[i], eta = 0.01, min_child_weight = xgb_params.iloc[i, 2],\n",
    "                      max_depth = xgb_params.iloc[i, 3], colsample_bytree = xgb_params.iloc[i, 4], subsample = xgb_params.iloc[i, 5],\n",
    "                      tree_method = 'gpu_hist', predictor= 'gpu_predictor')\n",
    "        xgb.set_params(**{'objective' : weighted_mse(j)})\n",
    "    \n",
    "        xgb.fit(x_train, y_train)\n",
    "        pred1 = xgb.predict(x_test)\n",
    "        score1 = SMAPE(y_test, pred1)\n",
    "        if score1 < score0:\n",
    "            best_alpha = j\n",
    "            score0 = score1\n",
    "    \n",
    "    alpha_list.append(best_alpha)\n",
    "    smape_list.append(score0)\n",
    "    print(\"building {} || best score : {} || alpha : {}\".format(i+1, score0, best_alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f84b587-e014-4259-a8da-ff5f9635895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_df = pd.DataFrame({'score':smape_list})\n",
    "plt.bar(np.arange(len(no_df))+1, no_df['score'])\n",
    "plt.plot([1,60], [10, 10], color = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98af3b9-4d1e-4f68-9b58-a46ecf821ea9",
   "metadata": {},
   "source": [
    "## 4. test inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92ef150-ad42-41c5-9085-35a222d4e8e9",
   "metadata": {},
   "source": [
    "### preprocessing for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "beginning-above",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>eta</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>subsample</th>\n",
       "      <th>best_it</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>610</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>610</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>463</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>434</td>\n",
       "      <td>0.01</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>434</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_estimators   eta  min_child_weight  max_depth  colsample_bytree  \\\n",
       "0           610  0.01                 1          4               0.9   \n",
       "1           610  0.01                 6          5               0.9   \n",
       "2           463  0.01                 3          3               0.9   \n",
       "3           434  0.01                 7          8               0.9   \n",
       "4           434  0.01                 5          6               0.9   \n",
       "\n",
       "   subsample  best_it  \n",
       "0        0.9      610  \n",
       "1        0.9      610  \n",
       "2        0.9      463  \n",
       "3        0.9      434  \n",
       "4        0.9      434  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xgb_params['alpha'] = alpha_list\n",
    "xgb_params['n_estimators'] = best_it\n",
    "xgb_params.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7edf93ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params.drop('best_it', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "middle-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params.to_csv('./hyperparameter_xgb_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "enormous-shelter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>eta</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>610</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>610</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>463</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>434</td>\n",
       "      <td>0.01</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>434</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_estimators   eta  min_child_weight  max_depth  colsample_bytree  \\\n",
       "0           610  0.01                 1          4               0.9   \n",
       "1           610  0.01                 6          5               0.9   \n",
       "2           463  0.01                 3          3               0.9   \n",
       "3           434  0.01                 7          8               0.9   \n",
       "4           434  0.01                 5          6               0.9   \n",
       "\n",
       "   subsample  \n",
       "0        0.9  \n",
       "1        0.9  \n",
       "2        0.9  \n",
       "3        0.9  \n",
       "4        0.9  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## best hyperparameters 불러오기\n",
    "xgb_params = pd.read_csv('smsle/hyperparameter_teacher.csv')\n",
    "xgb_params.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "constant-swimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = xgb_params['n_estimators'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d100944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n_estimators        200.00\n",
       "eta                   0.01\n",
       "min_child_weight      6.00\n",
       "max_depth             5.00\n",
       "colsample_bytree      0.90\n",
       "subsample             0.90\n",
       "best_it             610.00\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_params.iloc[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2b34453d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186677a26c1946e1997e80213012f687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 16\u001b[0m\n\u001b[0;32m      9\u001b[0m xgb \u001b[39m=\u001b[39m XGBRegressor(seed \u001b[39m=\u001b[39m seed, n_estimators \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mn_estimators\u001b[39m\u001b[39m'\u001b[39m], eta \u001b[39m=\u001b[39m \u001b[39m0.01\u001b[39m, \n\u001b[0;32m     10\u001b[0m                     min_child_weight \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mmin_child_weight\u001b[39m\u001b[39m'\u001b[39m], max_depth \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[0;32m     11\u001b[0m                     colsample_bytree \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mcolsample_bytree\u001b[39m\u001b[39m'\u001b[39m], subsample \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39msubsample\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     12\u001b[0m                     tree_method \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgpu_hist\u001b[39m\u001b[39m'\u001b[39m, predictor\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgpu_predictor\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m xgb_reg\u001b[39m.\u001b[39mset_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mreg:squaredlogerror\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[1;32m---> 16\u001b[0m xgb\u001b[39m.\u001b[39;49mfit(x_train, y_train)\n\u001b[0;32m     17\u001b[0m pred \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mpredict(x_train)\n\u001b[0;32m     19\u001b[0m preds \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m pred\n",
      "File \u001b[1;32mc:\\Users\\qja19\\Anaconda3\\envs\\xg\\lib\\site-packages\\xgboost\\sklearn.py:542\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[0;32m    539\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    540\u001b[0m         params\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39meval_metric\u001b[39m\u001b[39m'\u001b[39m: eval_metric})\n\u001b[1;32m--> 542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(params, train_dmatrix,\n\u001b[0;32m    543\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(), evals\u001b[39m=\u001b[39;49mevals,\n\u001b[0;32m    544\u001b[0m                       early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[0;32m    545\u001b[0m                       evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[0;32m    546\u001b[0m                       obj\u001b[39m=\u001b[39;49mobj, feval\u001b[39m=\u001b[39;49mfeval,\n\u001b[0;32m    547\u001b[0m                       verbose_eval\u001b[39m=\u001b[39;49mverbose, xgb_model\u001b[39m=\u001b[39;49mxgb_model,\n\u001b[0;32m    548\u001b[0m                       callbacks\u001b[39m=\u001b[39;49mcallbacks)\n\u001b[0;32m    550\u001b[0m \u001b[39mif\u001b[39;00m evals_result:\n\u001b[0;32m    551\u001b[0m     \u001b[39mfor\u001b[39;00m val \u001b[39min\u001b[39;00m evals_result\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\qja19\\Anaconda3\\envs\\xg\\lib\\site-packages\\xgboost\\training.py:208\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mif\u001b[39;00m evals_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(callback\u001b[39m.\u001b[39mrecord_evaluation(evals_result))\n\u001b[1;32m--> 208\u001b[0m \u001b[39mreturn\u001b[39;00m _train_internal(params, dtrain,\n\u001b[0;32m    209\u001b[0m                        num_boost_round\u001b[39m=\u001b[39;49mnum_boost_round,\n\u001b[0;32m    210\u001b[0m                        evals\u001b[39m=\u001b[39;49mevals,\n\u001b[0;32m    211\u001b[0m                        obj\u001b[39m=\u001b[39;49mobj, feval\u001b[39m=\u001b[39;49mfeval,\n\u001b[0;32m    212\u001b[0m                        xgb_model\u001b[39m=\u001b[39;49mxgb_model, callbacks\u001b[39m=\u001b[39;49mcallbacks)\n",
      "File \u001b[1;32mc:\\Users\\qja19\\Anaconda3\\envs\\xg\\lib\\site-packages\\xgboost\\training.py:63\u001b[0m, in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     56\u001b[0m callbacks_before_iter \u001b[39m=\u001b[39m [\n\u001b[0;32m     57\u001b[0m     cb \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks\n\u001b[0;32m     58\u001b[0m     \u001b[39mif\u001b[39;00m cb\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mbefore_iteration\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m)]\n\u001b[0;32m     59\u001b[0m callbacks_after_iter \u001b[39m=\u001b[39m [\n\u001b[0;32m     60\u001b[0m     cb \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks\n\u001b[0;32m     61\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m cb\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mbefore_iteration\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m)]\n\u001b[1;32m---> 63\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39;49m(start_iteration, num_boost_round):\n\u001b[0;32m     64\u001b[0m     \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks_before_iter:\n\u001b[0;32m     65\u001b[0m         cb(CallbackEnv(model\u001b[39m=\u001b[39mbst,\n\u001b[0;32m     66\u001b[0m                        cvfolds\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     67\u001b[0m                        iteration\u001b[39m=\u001b[39mi,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m                        rank\u001b[39m=\u001b[39mrank,\n\u001b[0;32m     71\u001b[0m                        evaluation_result_list\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "model_path = 'smsle/models'\n",
    "preds = []\n",
    "for i in tqdm(range(100)):\n",
    "    \n",
    "    y_train = train.loc[train.building_id == i+1, 'consumption']\n",
    "    x_train = train.loc[train.building_id == i+1, ].drop('consumption', axis=1)\n",
    "    params = xgb_params.iloc[i, :]\n",
    "\n",
    "    xgb = XGBRegressor(seed = seed, n_estimators = params['n_estimators'], eta = 0.01, \n",
    "                        min_child_weight = params['min_child_weight'], max_depth = params['max_depth'], \n",
    "                        colsample_bytree = params['colsample_bytree'], subsample = params['subsample'],\n",
    "                        tree_method = 'gpu_hist', predictor= 'gpu_predictor')\n",
    "\n",
    "    xgb_reg.set_params(**{'objective':'reg:squaredlogerror'})\n",
    "    \n",
    "    xgb.fit(x_train, y_train)\n",
    "    pred = xgb.predict(x_train)\n",
    "    \n",
    "    preds += pred\n",
    "\n",
    "    xgb.save_model(os.path.join(model_path, f'{i + 1}.model'))\n",
    "    # new_xgb_model.load_model(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d865cdab-bab7-4b4b-8a1d-e48c63609fa3",
   "metadata": {},
   "source": [
    "### seed ensemble\n",
    "#### - seed별로 예측값이 조금씩 바뀝니다. \n",
    "#### - seed의 영향을 제거하기 위해 6개의 seed(0부터 5)별로 훈련, 예측하여 6개 예측값의 평균을 구했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e1c328c-fad3-4d71-9f17-eec3302296c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4458e600ba1f488b875f7a16690fb58c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "\"['hour_mean', 'hour_std', 'day_hour_mean', 'Solar radiation'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m y_train \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39mloc[train\u001b[39m.\u001b[39mbuilding_id \u001b[39m==\u001b[39m i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mconsumption\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     13\u001b[0m x_train, x_test \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39mloc[train\u001b[39m.\u001b[39mbuilding_id \u001b[39m==\u001b[39m i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, ]\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39mconsumption\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), test\u001b[39m.\u001b[39mloc[test\u001b[39m.\u001b[39mbuilding_id \u001b[39m==\u001b[39m i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, ]\n\u001b[1;32m---> 14\u001b[0m x_test \u001b[39m=\u001b[39m x_test[x_train\u001b[39m.\u001b[39;49mcolumns]\n\u001b[0;32m     16\u001b[0m xgb \u001b[39m=\u001b[39m XGBRegressor(seed \u001b[39m=\u001b[39m seed, n_estimators \u001b[39m=\u001b[39m best_it[i], eta \u001b[39m=\u001b[39m \u001b[39m0.01\u001b[39m, \n\u001b[0;32m     17\u001b[0m                    min_child_weight \u001b[39m=\u001b[39m xgb_params\u001b[39m.\u001b[39miloc[i, \u001b[39m2\u001b[39m], max_depth \u001b[39m=\u001b[39m xgb_params\u001b[39m.\u001b[39miloc[i, \u001b[39m3\u001b[39m], \n\u001b[0;32m     18\u001b[0m                    colsample_bytree\u001b[39m=\u001b[39mxgb_params\u001b[39m.\u001b[39miloc[i, \u001b[39m4\u001b[39m], subsample\u001b[39m=\u001b[39mxgb_params\u001b[39m.\u001b[39miloc[i, \u001b[39m5\u001b[39m],\n\u001b[0;32m     19\u001b[0m                    tree_method \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgpu_hist\u001b[39m\u001b[39m'\u001b[39m, predictor\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgpu_predictor\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[39mif\u001b[39;00m xgb_params\u001b[39m.\u001b[39miloc[i,\u001b[39m6\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:  \u001b[39m# 만약 alpha가 0이 아니면 weighted_mse 사용\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\qja19\\Anaconda3\\envs\\xg\\lib\\site-packages\\pandas\\core\\frame.py:3030\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3028\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3029\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 3030\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloc\u001b[39m.\u001b[39;49m_get_listlike_indexer(key, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, raise_missing\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m   3032\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3033\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\qja19\\Anaconda3\\envs\\xg\\lib\\site-packages\\pandas\\core\\indexing.py:1266\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1263\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1264\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 1266\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_read_indexer(keyarr, indexer, axis, raise_missing\u001b[39m=\u001b[39;49mraise_missing)\n\u001b[0;32m   1267\u001b[0m \u001b[39mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[1;32mc:\\Users\\qja19\\Anaconda3\\envs\\xg\\lib\\site-packages\\pandas\\core\\indexing.py:1316\u001b[0m, in \u001b[0;36m_LocIndexer._validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1314\u001b[0m \u001b[39mif\u001b[39;00m raise_missing:\n\u001b[0;32m   1315\u001b[0m     not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(key) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(ax))\n\u001b[1;32m-> 1316\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1318\u001b[0m not_found \u001b[39m=\u001b[39m key[missing_mask]\n\u001b[0;32m   1320\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mdisplay.max_seq_items\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdisplay.width\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m80\u001b[39m):\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['hour_mean', 'hour_std', 'day_hour_mean', 'Solar radiation'] not in index\""
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('../data/preproc3_test.csv')\n",
    "test = test.drop('Time', axis=1)\n",
    "# test = test.drop('cluster', axis=1)\n",
    "# test = test.drop('type', axis=1)\n",
    "\n",
    "preds = np.array([]) \n",
    "for i in tqdm(range(100)):\n",
    "    \n",
    "    pred_df = pd.DataFrame()   # 시드별 예측값을 담을 data frame\n",
    "    \n",
    "    for seed in range(5): # 각 시드별 예측\n",
    "        y_train = train.loc[train.building_id == i+1, 'consumption']\n",
    "        x_train, x_test = train.loc[train.building_id == i+1, ].drop('consumption', axis=1), test.loc[test.building_id == i+1, ]\n",
    "        x_test = x_test[x_train.columns]\n",
    "        \n",
    "        xgb = XGBRegressor(seed = seed, n_estimators = best_it[i], eta = 0.01, \n",
    "                           min_child_weight = xgb_params.iloc[i, 2], max_depth = xgb_params.iloc[i, 3], \n",
    "                           colsample_bytree=xgb_params.iloc[i, 4], subsample=xgb_params.iloc[i, 5],\n",
    "                           tree_method = 'gpu_hist', predictor= 'gpu_predictor')\n",
    "    \n",
    "        if xgb_params.iloc[i,6] != 0:  # 만약 alpha가 0이 아니면 weighted_mse 사용\n",
    "            xgb.set_params(**{'objective':weighted_mse(xgb_params.iloc[i,6])})\n",
    "        \n",
    "        xgb.fit(x_train, y_train)\n",
    "        y_pred = xgb.predict(x_test)\n",
    "        pred_df.loc[:,seed] = y_pred   # 각 시드별 예측 담기\n",
    "        \n",
    "    pred = pred_df.mean(axis=1)        # (i+1)번째 건물의 예측 =  (i+1)번째 건물의 각 시드별 예측 평균값\n",
    "    preds = np.append(preds, pred)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-space",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = pd.Series(preds)\n",
    "\n",
    "fig, ax = plt.subplots(100, 1, figsize=(100,200), sharex = True)\n",
    "ax = ax.flatten()\n",
    "for i in range(100):\n",
    "    train_y = train.loc[train.building_id == i+1, 'consumption'].reset_index(drop = True)\n",
    "    test_y = preds[i*168:(i+1)*168]\n",
    "    ax[i].scatter(np.arange(2040) , train.loc[train.building_id == i+1, 'consumption'])\n",
    "    ax[i].scatter(np.arange(2040, 2040+168) , test_y)\n",
    "    ax[i].tick_params(axis='both', which='major', labelsize=6)\n",
    "    ax[i].tick_params(axis='both', which='minor', labelsize=4)\n",
    "#plt.savefig('./predict_xgb.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521eb6cf-f4bd-485f-8f30-3d442c22d214",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../data/sample_submission.csv')\n",
    "submission['answer'] = preds\n",
    "submission.to_csv('../data/submission/submission_xgb_noclip.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95d9483-009b-4a0c-9056-394008bbc3b6",
   "metadata": {},
   "source": [
    "## 5. post processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf1ac57-34ef-43cd-9b94-273fa107bfa4",
   "metadata": {},
   "source": [
    "#### weighted mse와 같은 맥락에서, 과도한 underestimate를 막기 위해 예측값을 후처리했습니다.\n",
    "##### - 예측 주로부터 직전 4주(train set 마지막 28일)의 건물별 요일별 시간대별 전력소비량의 최솟값을 구한 뒤, \n",
    "##### - test set의 같은 건물 요일 시간대의 예측값과 비교하여 만약 1번의 최솟값보다 예측값이 작다면 최솟값으로 예측값을 대체해주었습니다.\n",
    "##### - public score 0.01 , private score 0.08 정도의 성능 향상이 있었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d24af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_to_post = pd.read_csv('../data/preproc_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dd6a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_to_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e247ae6-fa0c-40c6-944a-cb988872e0c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cols = ['num', 'date_time', 'power', 'temp', 'wind','hum' ,'prec', 'sun', 'non_elec', 'solar']\n",
    "\n",
    "# train_to_post.columns = cols\n",
    "date = pd.to_datetime(train_to_post.Time)\n",
    "train_to_post['hour'] = date.dt.hour\n",
    "train_to_post['day'] = date.dt.weekday\n",
    "train_to_post['month'] = date.dt.month\n",
    "train_to_post['week'] = date.dt.weekofyear\n",
    "train_to_post = train_to_post.loc[(('2022-08-17'>train_to_post.Time)|(train_to_post.Time>='2022-08-18')), ].reset_index(drop = True)\n",
    "\n",
    "pred_clip = []\n",
    "test_to_post = pd.read_csv('../data/preproc_test.csv',  encoding = 'cp949')\n",
    "# cols = ['num', 'date_time', 'temp', 'wind','hum' ,'prec', 'sun', 'non_elec', 'solar']\n",
    "# test_to_post.columns = cols\n",
    "date = pd.to_datetime(test_to_post.Time)\n",
    "test_to_post['hour'] = date.dt.hour\n",
    "test_to_post['day'] = date.dt.weekday\n",
    "test_to_post['month'] = date.dt.month\n",
    "test_to_post['week'] = date.dt.weekofyear\n",
    "\n",
    "## submission 불러오기\n",
    "df = pd.read_csv('../data/submission/submission_xgb_noclip5.csv')\n",
    "for i in range(100):\n",
    "    min_data = train_to_post.loc[train_to_post.building_id == i+1, ].iloc[-28*24:, :] ## 건물별로 직전 28일의 데이터 불러오기\n",
    "    ## 요일별, 시간대별 최솟값 계산\n",
    "    min_data = pd.pivot_table(min_data, values = 'consumption', index = ['day', 'hour'], aggfunc = min).reset_index() \n",
    "    pred = df.answer[168*i:168*(i+1)].reset_index(drop=True) ## 168개 데이터, 즉 건물별 예측값 불러오기\n",
    "    day =  test_to_post.day[168*i:168*(i+1)].reset_index(drop=True) ## 예측값 요일 불러오기\n",
    "    hour = test_to_post.hour[168*i:168*(i+1)].reset_index(drop=True) ## 예측값 시간 불러오기\n",
    "    df_pred = pd.concat([pred, day, hour], axis = 1)\n",
    "    df_pred.columns = ['pred', 'day', 'hour']\n",
    "    for j in range(len(df_pred)):\n",
    "        min_consumption = min_data.loc[(min_data.day == df_pred.day[j])&(min_data.hour == df_pred.hour[j]), 'consumption'].values[0]\n",
    "        if df_pred.pred[j] < min_consumption:\n",
    "            pred_clip.append(min_consumption)\n",
    "        else:\n",
    "            pred_clip.append(df_pred.pred[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6ee069-c7e2-4819-bbf6-f6c9a44cd4d1",
   "metadata": {},
   "source": [
    "##### 초록색으로 표시된 값이 원래의 예측값, 주황색이 후처리된 예측값입니다.\n",
    "##### 변동이 거의 없는 건물도 있으나, 유의미하게 바뀐 건물도 확인됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-saturn",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_origin = df.answer\n",
    "pred_clip = pd.Series(pred_clip)\n",
    "\n",
    "for i in range(100):\n",
    "    power = train_to_post.loc[train_to_post.building_id == i+1, 'consumption'].reset_index(drop=True)\n",
    "    preds = pred_clip[i*168:(i+1)*168]\n",
    "    preds_origin = pred_origin[i*168:(i+1)*168]\n",
    "    preds.index = range(power.index[-1], power.index[-1]+168)\n",
    "    preds_origin.index = range(power.index[-1], power.index[-1]+168)\n",
    "    \n",
    "    plot_series(power, preds,  preds_origin, markers = [',', ',', ','])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22adefcf-a346-4ac9-b2af-4d23f2c2365f",
   "metadata": {},
   "source": [
    "#### create submission file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../data/sample_submission.csv')\n",
    "submission['answer'] = pred_clip\n",
    "submission.to_csv('../data/submission/submission_xgb_final.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f66185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
