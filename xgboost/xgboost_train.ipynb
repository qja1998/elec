{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01de2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- Python & library version --------------------------\n",
    "# Python version: 3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]\n",
    "# pandas version: 1.2.4\n",
    "# numpy version: 1.19.2\n",
    "# matplotlib version: 3.3.4\n",
    "# tqdm version: 4.59.0\n",
    "# sktime version: 0.6.1\n",
    "# xgboost version: 1.2.1\n",
    "# seaborn version: 0.11.1\n",
    "# scikit-learn version: 0.24.2\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "civilian-stations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- Python & library version --------------------------\n",
      "Python version: 3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]\n",
      "pandas version: 1.2.4\n",
      "numpy version: 1.19.2\n",
      "matplotlib version: 3.3.4\n",
      "tqdm version: 4.59.0\n",
      "sktime version: 0.6.1\n",
      "xgboost version: 1.2.1\n",
      "seaborn version: 0.11.1\n",
      "scikit-learn version: 0.24.2\n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import sktime\n",
    "import tqdm as tq\n",
    "import xgboost as xgb\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "print(\"-------------------------- Python & library version --------------------------\")\n",
    "print(\"Python version: {}\".format(sys.version))\n",
    "print(\"pandas version: {}\".format(pd.__version__))\n",
    "print(\"numpy version: {}\".format(np.__version__))\n",
    "print(\"matplotlib version: {}\".format(matplotlib.__version__))\n",
    "print(\"tqdm version: {}\".format(tq.__version__))\n",
    "print(\"sktime version: {}\".format(sktime.__version__))\n",
    "print(\"xgboost version: {}\".format(xgb.__version__))\n",
    "print(\"seaborn version: {}\".format(sns.__version__))\n",
    "print(\"scikit-learn version: {}\".format(skl.__version__))\n",
    "print(\"------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6b5e25e-f9a1-4980-ac53-6cdb73b3d0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.utils.plotting import plot_series\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "pd.set_option('display.max_columns', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83e7738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/preproc3_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c414fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('Time', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffe02f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['building_id', 'sin_time', 'cos_time', 'day', 'month', 'day_hour_mean',\n",
       "       'hour_mean', 'hour_std', 'holiday', 'Temperature', 'Precipitation',\n",
       "       'Wind speed', 'Humidity', 'DI', 'Solar radiation', 'consumption',\n",
       "       'CDH'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d356ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_dim = 1\n",
    "\n",
    "# class Regressor(torch.nn.Module):\n",
    "#     def __init__(self, input_size, em_size=16, output_size=12, a=10):\n",
    "#         super().__init__()\n",
    "#         self.linear = nn.Sequential(\n",
    "#             nn.Linear(input_size, em_size),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(em_size, output_size),\n",
    "#             nn.Softmax(),\n",
    "#         )\n",
    "#         self.reg = XGBRegressor(n_estimators=a, tree_method='gpu_hist', objective = 'multi:softprob')\n",
    "\n",
    "#     def forward(self, x, y, train=True):\n",
    "#         x = self.linear(x)\n",
    "        \n",
    "#         if train:\n",
    "#           self.reg.fit(x.detach().cpu().numpy(), y.detach().cpu().numpy())\n",
    "          \n",
    "#         out = self.reg.predict(x.detach().cpu().numpy())\n",
    "#         out = torch.tensor(out)\n",
    "\n",
    "#         return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dabc56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train embedding models\n",
    "# t_input = len(train.columns) - 1\n",
    "# s_input = len(train.columns) - 2\n",
    "# output_size = 15\n",
    "# epochs = 100\n",
    "\n",
    "# t_embedding = EmbeddingModel(t_input)\n",
    "# s_embedding = EmbeddingModel(s_input)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = nn.optim.Adam(s_embedding.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "personal-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SMAPE loss function\n",
    "def SMAPE(true, pred):\n",
    "    return np.mean((np.abs(true-pred))/(np.abs(true) + np.abs(pred))) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c131d074-ca9f-4c1b-824b-9899c4442c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### alpha를 argument로 받는 함수로 실제 objective function을 wrapping하여 alpha값을 쉽게 조정할 수 있도록 작성했습니다.\n",
    "# custom objective function for forcing model not to underestimate\n",
    "def weighted_mse(alpha = 1):\n",
    "    def weighted_mse_fixed(label, pred):\n",
    "        residual = (label - pred).astype(\"float\")\n",
    "        grad = np.where(residual>0, -2*alpha*residual, -2*residual)\n",
    "        hess = np.where(residual>0, 2*alpha, 2.0)\n",
    "        return grad, hess\n",
    "    return weighted_mse_fixed\n",
    "\n",
    "def log_mae(label, pred):\n",
    "    ae = np.log1p(label) - np.log1p(pred)\n",
    "    grad = -ae / (label * abs(ae))\n",
    "    hess = ae / (label ** 2 * abs(ae))\n",
    "    return grad, hess\n",
    "\n",
    "def rmsle_loss(y_pred, y_true):\n",
    "    return 'rmsle', np.sqrt(np.mean((np.log1p(y_pred) - np.log1p(y_true))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcca426c-ed3a-4bd2-b0fe-c700b29c080b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47791449e5e942dc83134f0cf87d647f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n",
      "{'colsample_bytree': 0.9, 'eta': 0.01, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Building 1 SMAPE : 8.153339110837528\n",
      "Fitting 1 folds for each of 168 candidates, totalling 168 fits\n"
     ]
    }
   ],
   "source": [
    "## gridsearchCV for best model : 대략 1시간 소요\n",
    "\n",
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "smape = make_scorer(SMAPE, greater_is_better = False)\n",
    "\n",
    "df = pd.DataFrame(columns = ['n_estimators', 'eta', 'min_child_weight','max_depth', 'colsample_bytree', 'subsample'])\n",
    "preds = np.array([])\n",
    "\n",
    "grid = {'n_estimators' : [200], 'eta' : [0.01], 'min_child_weight' : np.arange(1, 8, 1), \n",
    "        'max_depth' : np.arange(3,9,1) , 'colsample_bytree' :np.arange(0.8, 1.0, 0.1), \n",
    "        'subsample' :np.arange(0.8, 1.0, 0.1)} # fix the n_estimators & eta(learning rate)\n",
    "        \n",
    "for i in tqdm(np.arange(1, 101)):\n",
    "    y = train.loc[train.building_id == i, 'consumption']\n",
    "    x = train.loc[train.building_id == i, ].drop('consumption', axis=1)\n",
    "    y_train, y_test, x_train, x_test = temporal_train_test_split(y = y, X = x, test_size = 168)\n",
    "    \n",
    "    \n",
    "    pds = PredefinedSplit(np.append(-np.ones(len(x_train)-168), np.zeros(168)))\n",
    "    gcv = GridSearchCV(estimator = XGBRegressor(seed = 0, gpu_id = 0, \n",
    "                                                tree_method = 'gpu_hist', predictor= 'gpu_predictor'),\n",
    "                       param_grid = grid, scoring = smape, cv=pds, refit = True, verbose = True)\n",
    "    \n",
    "    \n",
    "    gcv.fit(x_train, y_train)\n",
    "    best = gcv.best_estimator_\n",
    "    params = gcv.best_params_\n",
    "    print(params)\n",
    "    pred = best.predict(x_test)\n",
    "    print(f'Building {i} SMAPE : {SMAPE(y_test, pred)}')\n",
    "    preds = np.append(preds, pred)\n",
    "    df = pd.concat([df, pd.DataFrame(params, index = [0])], axis = 0)\n",
    "df.to_csv('./hyperparameter_teacher1.csv', index = False) # save the tuned parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5696e3-5111-4e09-ad31-1156aa6af19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = pd.read_csv('./hyperparameter_teacher1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd0bf02-4a3b-4c0c-aa91-d4580d4773d3",
   "metadata": {},
   "source": [
    "### find the bset iteration (given alpha = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-chain",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = []   # smape 값을 저장할 list\n",
    "best_it = []  # best interation을 저장할 list\n",
    "for i in tqdm(range(100)):\n",
    "    y = train.loc[train.building_id == i+1, 'consumption']\n",
    "    x = train.loc[train.building_id == i+1, ].drop('consumption', axis=1)\n",
    "    y_train, y_valid, x_train, x_valid = temporal_train_test_split(y = y, X = x, test_size = 168)\n",
    "    \n",
    "    xgb_reg = XGBRegressor(n_estimators = 20000, eta = 0.01, min_child_weight = xgb_params.iloc[i, 2],\n",
    "                           max_depth = xgb_params.iloc[i, 3], colsample_bytree = xgb_params.iloc[i, 4], \n",
    "                           subsample = xgb_params.iloc[i, 5], seed=0,\n",
    "                           tree_method = 'gpu_hist', predictor= 'gpu_predictor')\n",
    "    # xgb_reg.set_params(**{'objective':weighted_mse(100)}) # alpha = 100으로 고정\n",
    "    xgb_reg.set_params(**{'objective':'reg:squaredlogerror'}) # SMSLE\n",
    "    xgb_reg.fit(x_train, y_train, eval_set=[(x_train, y_train), \n",
    "                                            (x_valid, y_valid)], early_stopping_rounds=3000, verbose=False)\n",
    "    y_pred = xgb_reg.predict(x_valid)\n",
    "    pred = pd.Series(y_pred)   \n",
    "    \n",
    "    sm = SMAPE(y_valid, y_pred)\n",
    "    scores.append(sm)\n",
    "    # print(xgb_reg.best_iteration, sm)\n",
    "    best_it.append(xgb_reg.best_iteration) ## 실제 best iteration은 이 값에 +1 해주어야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-copper",
   "metadata": {},
   "source": [
    "### alpha tuning for weighted MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-plaza",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = []\n",
    "smape_list = []\n",
    "for i in tqdm(range(100)):\n",
    "    y = train.loc[train.building_id == i+1, 'consumption']\n",
    "    x = train.loc[train.building_id == i+1, ].drop('consumption', axis=1)\n",
    "    y_train, y_test, x_train, x_test = temporal_train_test_split(y = y, X = x, test_size = 168)\n",
    "    xgb = XGBRegressor(seed = 0,\n",
    "                      n_estimators = best_it[i], eta = 0.01, min_child_weight = xgb_params.iloc[i, 2],\n",
    "                      max_depth = xgb_params.iloc[i, 3], colsample_bytree = xgb_params.iloc[i, 4], subsample = xgb_params.iloc[i, 5],\n",
    "                      tree_method = 'gpu_hist', predictor= 'gpu_predictor')\n",
    "    \n",
    "    xgb.fit(x_train, y_train)\n",
    "    pred0 = xgb.predict(x_test)\n",
    "    best_alpha = 0\n",
    "    score0 = SMAPE(y_test,pred0)\n",
    "    \n",
    "    for j in [1, 3, 5, 7, 10, 25, 50, 75, 100]:\n",
    "        xgb = XGBRegressor(seed = 0,\n",
    "                      n_estimators = best_it[i], eta = 0.01, min_child_weight = xgb_params.iloc[i, 2],\n",
    "                      max_depth = xgb_params.iloc[i, 3], colsample_bytree = xgb_params.iloc[i, 4], subsample = xgb_params.iloc[i, 5],\n",
    "                      tree_method = 'gpu_hist', predictor= 'gpu_predictor')\n",
    "        xgb.set_params(**{'objective' : weighted_mse(j)})\n",
    "    \n",
    "        xgb.fit(x_train, y_train)\n",
    "        pred1 = xgb.predict(x_test)\n",
    "        score1 = SMAPE(y_test, pred1)\n",
    "        if score1 < score0:\n",
    "            best_alpha = j\n",
    "            score0 = score1\n",
    "    \n",
    "    alpha_list.append(best_alpha)\n",
    "    smape_list.append(score0)\n",
    "    print(\"building {} || best score : {} || alpha : {}\".format(i+1, score0, best_alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f84b587-e014-4259-a8da-ff5f9635895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_df = pd.DataFrame({'score':smape_list})\n",
    "plt.bar(np.arange(len(no_df))+1, no_df['score'])\n",
    "plt.plot([1,60], [10, 10], color = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98af3b9-4d1e-4f68-9b58-a46ecf821ea9",
   "metadata": {},
   "source": [
    "## 4. test inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92ef150-ad42-41c5-9085-35a222d4e8e9",
   "metadata": {},
   "source": [
    "### preprocessing for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-above",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params['alpha'] = alpha_list\n",
    "xgb_params['best_it'] = best_it\n",
    "xgb_params.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params.to_csv('./hyperparameter_xgb_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "## best hyperparameters 불러오기\n",
    "xgb_params = pd.read_csv('./hyperparameter_xgb_final.csv')\n",
    "xgb_params.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-swimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_it = xgb_params['best_it'].to_list()\n",
    "best_it[0]        # 1051"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d865cdab-bab7-4b4b-8a1d-e48c63609fa3",
   "metadata": {},
   "source": [
    "### seed ensemble\n",
    "#### - seed별로 예측값이 조금씩 바뀝니다. \n",
    "#### - seed의 영향을 제거하기 위해 6개의 seed(0부터 5)별로 훈련, 예측하여 6개 예측값의 평균을 구했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1c328c-fad3-4d71-9f17-eec3302296c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/preproc3_test.csv')\n",
    "test = test.drop('Time', axis=1)\n",
    "# test = test.drop('cluster', axis=1)\n",
    "# test = test.drop('type', axis=1)\n",
    "\n",
    "preds = np.array([]) \n",
    "for i in tqdm(range(100)):\n",
    "    \n",
    "    pred_df = pd.DataFrame()   # 시드별 예측값을 담을 data frame\n",
    "    \n",
    "    for seed in range(5): # 각 시드별 예측\n",
    "        y_train = train.loc[train.building_id == i+1, 'consumption']\n",
    "        x_train, x_test = train.loc[train.building_id == i+1, ].drop('consumption', axis=1), test.loc[test.building_id == i+1, ]\n",
    "        x_test = x_test[x_train.columns]\n",
    "        \n",
    "        xgb = XGBRegressor(seed = seed, n_estimators = best_it[i], eta = 0.01, \n",
    "                           min_child_weight = xgb_params.iloc[i, 2], max_depth = xgb_params.iloc[i, 3], \n",
    "                           colsample_bytree=xgb_params.iloc[i, 4], subsample=xgb_params.iloc[i, 5],\n",
    "                           tree_method = 'gpu_hist', predictor= 'gpu_predictor')\n",
    "    \n",
    "        if xgb_params.iloc[i,6] != 0:  # 만약 alpha가 0이 아니면 weighted_mse 사용\n",
    "            xgb.set_params(**{'objective':weighted_mse(xgb_params.iloc[i,6])})\n",
    "        \n",
    "        xgb.fit(x_train, y_train)\n",
    "        y_pred = xgb.predict(x_test)\n",
    "        pred_df.loc[:,seed] = y_pred   # 각 시드별 예측 담기\n",
    "        \n",
    "    pred = pred_df.mean(axis=1)        # (i+1)번째 건물의 예측 =  (i+1)번째 건물의 각 시드별 예측 평균값\n",
    "    preds = np.append(preds, pred)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-space",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = pd.Series(preds)\n",
    "\n",
    "fig, ax = plt.subplots(100, 1, figsize=(100,200), sharex = True)\n",
    "ax = ax.flatten()\n",
    "for i in range(100):\n",
    "    train_y = train.loc[train.building_id == i+1, 'consumption'].reset_index(drop = True)\n",
    "    test_y = preds[i*168:(i+1)*168]\n",
    "    ax[i].scatter(np.arange(2040) , train.loc[train.building_id == i+1, 'consumption'])\n",
    "    ax[i].scatter(np.arange(2040, 2040+168) , test_y)\n",
    "    ax[i].tick_params(axis='both', which='major', labelsize=6)\n",
    "    ax[i].tick_params(axis='both', which='minor', labelsize=4)\n",
    "#plt.savefig('./predict_xgb.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521eb6cf-f4bd-485f-8f30-3d442c22d214",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../data/sample_submission.csv')\n",
    "submission['answer'] = preds\n",
    "submission.to_csv('../data/submission/submission_xgb_noclip.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95d9483-009b-4a0c-9056-394008bbc3b6",
   "metadata": {},
   "source": [
    "## 5. post processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf1ac57-34ef-43cd-9b94-273fa107bfa4",
   "metadata": {},
   "source": [
    "#### weighted mse와 같은 맥락에서, 과도한 underestimate를 막기 위해 예측값을 후처리했습니다.\n",
    "##### - 예측 주로부터 직전 4주(train set 마지막 28일)의 건물별 요일별 시간대별 전력소비량의 최솟값을 구한 뒤, \n",
    "##### - test set의 같은 건물 요일 시간대의 예측값과 비교하여 만약 1번의 최솟값보다 예측값이 작다면 최솟값으로 예측값을 대체해주었습니다.\n",
    "##### - public score 0.01 , private score 0.08 정도의 성능 향상이 있었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d24af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_to_post = pd.read_csv('../data/preproc_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dd6a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_to_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e247ae6-fa0c-40c6-944a-cb988872e0c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cols = ['num', 'date_time', 'power', 'temp', 'wind','hum' ,'prec', 'sun', 'non_elec', 'solar']\n",
    "\n",
    "# train_to_post.columns = cols\n",
    "date = pd.to_datetime(train_to_post.Time)\n",
    "train_to_post['hour'] = date.dt.hour\n",
    "train_to_post['day'] = date.dt.weekday\n",
    "train_to_post['month'] = date.dt.month\n",
    "train_to_post['week'] = date.dt.weekofyear\n",
    "train_to_post = train_to_post.loc[(('2022-08-17'>train_to_post.Time)|(train_to_post.Time>='2022-08-18')), ].reset_index(drop = True)\n",
    "\n",
    "pred_clip = []\n",
    "test_to_post = pd.read_csv('../data/preproc_test.csv',  encoding = 'cp949')\n",
    "# cols = ['num', 'date_time', 'temp', 'wind','hum' ,'prec', 'sun', 'non_elec', 'solar']\n",
    "# test_to_post.columns = cols\n",
    "date = pd.to_datetime(test_to_post.Time)\n",
    "test_to_post['hour'] = date.dt.hour\n",
    "test_to_post['day'] = date.dt.weekday\n",
    "test_to_post['month'] = date.dt.month\n",
    "test_to_post['week'] = date.dt.weekofyear\n",
    "\n",
    "## submission 불러오기\n",
    "df = pd.read_csv('../data/submission/submission_xgb_noclip5.csv')\n",
    "for i in range(100):\n",
    "    min_data = train_to_post.loc[train_to_post.building_id == i+1, ].iloc[-28*24:, :] ## 건물별로 직전 28일의 데이터 불러오기\n",
    "    ## 요일별, 시간대별 최솟값 계산\n",
    "    min_data = pd.pivot_table(min_data, values = 'consumption', index = ['day', 'hour'], aggfunc = min).reset_index() \n",
    "    pred = df.answer[168*i:168*(i+1)].reset_index(drop=True) ## 168개 데이터, 즉 건물별 예측값 불러오기\n",
    "    day =  test_to_post.day[168*i:168*(i+1)].reset_index(drop=True) ## 예측값 요일 불러오기\n",
    "    hour = test_to_post.hour[168*i:168*(i+1)].reset_index(drop=True) ## 예측값 시간 불러오기\n",
    "    df_pred = pd.concat([pred, day, hour], axis = 1)\n",
    "    df_pred.columns = ['pred', 'day', 'hour']\n",
    "    for j in range(len(df_pred)):\n",
    "        min_consumption = min_data.loc[(min_data.day == df_pred.day[j])&(min_data.hour == df_pred.hour[j]), 'consumption'].values[0]\n",
    "        if df_pred.pred[j] < min_consumption:\n",
    "            pred_clip.append(min_consumption)\n",
    "        else:\n",
    "            pred_clip.append(df_pred.pred[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6ee069-c7e2-4819-bbf6-f6c9a44cd4d1",
   "metadata": {},
   "source": [
    "##### 초록색으로 표시된 값이 원래의 예측값, 주황색이 후처리된 예측값입니다.\n",
    "##### 변동이 거의 없는 건물도 있으나, 유의미하게 바뀐 건물도 확인됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-saturn",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_origin = df.answer\n",
    "pred_clip = pd.Series(pred_clip)\n",
    "\n",
    "for i in range(100):\n",
    "    power = train_to_post.loc[train_to_post.building_id == i+1, 'consumption'].reset_index(drop=True)\n",
    "    preds = pred_clip[i*168:(i+1)*168]\n",
    "    preds_origin = pred_origin[i*168:(i+1)*168]\n",
    "    preds.index = range(power.index[-1], power.index[-1]+168)\n",
    "    preds_origin.index = range(power.index[-1], power.index[-1]+168)\n",
    "    \n",
    "    plot_series(power, preds,  preds_origin, markers = [',', ',', ','])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22adefcf-a346-4ac9-b2af-4d23f2c2365f",
   "metadata": {},
   "source": [
    "#### create submission file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../data/sample_submission.csv')\n",
    "submission['answer'] = pred_clip\n",
    "submission.to_csv('../data/submission/submission_xgb_final.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f66185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
